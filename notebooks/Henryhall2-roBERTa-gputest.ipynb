{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check gpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /home/hamish/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[name: \"/device:CPU:0\"\n",
      "device_type: \"CPU\"\n",
      "memory_limit: 268435456\n",
      "locality {\n",
      "}\n",
      "incarnation: 10375747644405532618\n",
      ", name: \"/device:XLA_CPU:0\"\n",
      "device_type: \"XLA_CPU\"\n",
      "memory_limit: 17179869184\n",
      "locality {\n",
      "}\n",
      "incarnation: 1232235851224694781\n",
      "physical_device_desc: \"device: XLA_CPU device\"\n",
      ", name: \"/device:XLA_GPU:0\"\n",
      "device_type: \"XLA_GPU\"\n",
      "memory_limit: 17179869184\n",
      "locality {\n",
      "}\n",
      "incarnation: 8465212442591159272\n",
      "physical_device_desc: \"device: XLA_GPU device\"\n",
      ", name: \"/device:GPU:0\"\n",
      "device_type: \"GPU\"\n",
      "memory_limit: 22504868480\n",
      "locality {\n",
      "  bus_id: 1\n",
      "  links {\n",
      "  }\n",
      "}\n",
      "incarnation: 6721925694990093307\n",
      "physical_device_desc: \"device: 0, name: TITAN RTX, pci bus id: 0000:42:00.0, compute capability: 7.5\"\n",
      "]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.python.client import device_lib\n",
    "print(device_lib.list_local_devices())\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "gpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.config.list_physical_devices('GPU')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Roberta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# get data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nest_asyncio\n",
    "import sys\n",
    "from green_mood_tracker.twint_class import TWINT\n",
    "import pandas as pd\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk import download\n",
    "from green_mood_tracker.training_data import get_raw_data_notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hamish/.virtualenvs/henry2/lib/python3.6/site-packages/green_mood_tracker/training_data.py:24: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  sentiment140_final['source'] = 'sentiment140'\n"
     ]
    }
   ],
   "source": [
    "raw_data = get_raw_data_notebook()\n",
    "greenenergytest = pd.read_csv(\"../raw_data/greenenergytest.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/hamish/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hamish/.virtualenvs/henry2/lib/python3.6/site-packages/green_mood_tracker/training_data.py:24: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  sentiment140_final['source'] = 'sentiment140'\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>polarity</th>\n",
       "      <th>source</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1467933112</td>\n",
       "      <td>angel going miss athlete weekend</td>\n",
       "      <td>0</td>\n",
       "      <td>sts_gold</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2323395086</td>\n",
       "      <td>look though shaq getting traded cleveland play...</td>\n",
       "      <td>0</td>\n",
       "      <td>sts_gold</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1467968979</td>\n",
       "      <td>april th isnt coming soon enough</td>\n",
       "      <td>0</td>\n",
       "      <td>sts_gold</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1990283756</td>\n",
       "      <td>drinking mcdonalds coffee understanding someon...</td>\n",
       "      <td>0</td>\n",
       "      <td>sts_gold</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1988884918</td>\n",
       "      <td>dissapointed taylor swift doesnt twitter</td>\n",
       "      <td>0</td>\n",
       "      <td>sts_gold</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           id                                               text  polarity  \\\n",
       "0  1467933112                   angel going miss athlete weekend         0   \n",
       "1  2323395086  look though shaq getting traded cleveland play...         0   \n",
       "2  1467968979                   april th isnt coming soon enough         0   \n",
       "3  1990283756  drinking mcdonalds coffee understanding someon...         0   \n",
       "4  1988884918           dissapointed taylor swift doesnt twitter         0   \n",
       "\n",
       "     source  \n",
       "0  sts_gold  \n",
       "1  sts_gold  \n",
       "2  sts_gold  \n",
       "3  sts_gold  \n",
       "4  sts_gold  "
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from green_mood_tracker.data_cleaning import clean\n",
    "raw_data = get_raw_data_notebook()\n",
    "raw_data_clean = clean(raw_data,'text')\n",
    "greenenergytest_clean = clean(greenenergytest,'text')\n",
    "raw_data_clean.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>polarity</th>\n",
       "      <th>polarity-neutral</th>\n",
       "      <th>Annotator</th>\n",
       "      <th>0=negative, 1=positive</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.331530e+18</td>\n",
       "      <td>yeah green energy great httpstcodxasquuhhq</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>Henry</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.331530e+18</td>\n",
       "      <td>yes many job created green energy entire crowd...</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>Henry</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.331070e+18</td>\n",
       "      <td>green energy generally</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Henry</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.330080e+18</td>\n",
       "      <td>nuri elf us astral magic create portal attack ...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Henry</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.329920e+18</td>\n",
       "      <td>httpstcoanwhmnrbqa hey comment china shuts sel...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Henry</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             id                                               text  polarity  \\\n",
       "0  1.331530e+18         yeah green energy great httpstcodxasquuhhq         1   \n",
       "1  1.331530e+18  yes many job created green energy entire crowd...         1   \n",
       "2  1.331070e+18                             green energy generally         1   \n",
       "3  1.330080e+18  nuri elf us astral magic create portal attack ...         1   \n",
       "4  1.329920e+18  httpstcoanwhmnrbqa hey comment china shuts sel...         0   \n",
       "\n",
       "   polarity-neutral Annotator  0=negative, 1=positive  \n",
       "0                 2     Henry                     NaN  \n",
       "1                 2     Henry                     NaN  \n",
       "2                 1     Henry                     NaN  \n",
       "3                 1     Henry                     NaN  \n",
       "4                 0     Henry                     NaN  "
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "greenenergytest_clean.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    14880\n",
       "2    10836\n",
       "0    10756\n",
       "Name: polarity, dtype: int64"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_data_clean.polarity.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    2333\n",
       "0     572\n",
       "2     519\n",
       "Name: polarity, dtype: int64"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_data_clean[raw_data_clean['source']=='twitter_corpus'].polarity.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data_clean = raw_data_clean[raw_data_clean['polarity']!=1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_sample = raw_data_clean.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data_sample = raw_data_clean.sample(n=100_000,random_state=0).reset_index()\n",
    "#data_sample.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>polarity</th>\n",
       "      <th>source</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1467933112</td>\n",
       "      <td>angel going miss athlete weekend</td>\n",
       "      <td>0</td>\n",
       "      <td>sts_gold</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2323395086</td>\n",
       "      <td>look though shaq getting traded cleveland play...</td>\n",
       "      <td>0</td>\n",
       "      <td>sts_gold</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1467968979</td>\n",
       "      <td>april th isnt coming soon enough</td>\n",
       "      <td>0</td>\n",
       "      <td>sts_gold</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1990283756</td>\n",
       "      <td>drinking mcdonalds coffee understanding someon...</td>\n",
       "      <td>0</td>\n",
       "      <td>sts_gold</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1988884918</td>\n",
       "      <td>dissapointed taylor swift doesnt twitter</td>\n",
       "      <td>0</td>\n",
       "      <td>sts_gold</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           id                                               text  polarity  \\\n",
       "0  1467933112                   angel going miss athlete weekend         0   \n",
       "1  2323395086  look though shaq getting traded cleveland play...         0   \n",
       "2  1467968979                   april th isnt coming soon enough         0   \n",
       "3  1990283756  drinking mcdonalds coffee understanding someon...         0   \n",
       "4  1988884918           dissapointed taylor swift doesnt twitter         0   \n",
       "\n",
       "     source  \n",
       "0  sts_gold  \n",
       "1  sts_gold  \n",
       "2  sts_gold  \n",
       "3  sts_gold  \n",
       "4  sts_gold  "
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_sample['polarity'] = data_sample.polarity.map({2:1,0:0})\n",
    "data_sample.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 21592 entries, 0 to 37000\n",
      "Data columns (total 4 columns):\n",
      " #   Column    Non-Null Count  Dtype \n",
      "---  ------    --------------  ----- \n",
      " 0   id        21592 non-null  object\n",
      " 1   text      21592 non-null  object\n",
      " 2   polarity  21592 non-null  int64 \n",
      " 3   source    21592 non-null  object\n",
      "dtypes: int64(1), object(3)\n",
      "memory usage: 843.4+ KB\n"
     ]
    }
   ],
   "source": [
    "data_sample.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    10836\n",
       "0    10756\n",
       "Name: polarity, dtype: int64"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_sample.polarity.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_sample['text'] = data_sample.text.apply((lambda x: x.split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = data_sample.text\n",
    "y = data_sample.polarity\n",
    "\n",
    "sentence_train, sentence_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_train, sentence_val, y_train, y_val = train_test_split(sentence_train, y_train, test_size=0.3, random_state = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install absl-py --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#!pip install tensorflow-datasets --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install transformers --quiet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# build roberta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "# Recommended tensorflow version is <= 2.1.0, otherwise F1 score function breaks\n",
    "import tensorflow as tf\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow_datasets as tfds\n",
    "from transformers import TFRobertaForSequenceClassification, TFRobertaModel\n",
    "from transformers import RobertaTokenizerFast\n",
    "from transformers import AdamWeightDecay\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "roberta_tokenizer = RobertaTokenizerFast.from_pretrained(\"distilroberta-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = 50\n",
    "\n",
    "# the recommended batches size for BERT are 32,64 ... however on this dataset we are overfitting quite fast\n",
    "# and smaller batches work like a regularization.\n",
    "# You might play with adding another dropout layer instead.\n",
    "\n",
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_example_to_feature(review):\n",
    "    # combine step for tokenization, WordPiece vector mapping and will\n",
    "    # add also special tokens and truncate reviews longer than our max length\n",
    "    return roberta_tokenizer.encode_plus(review,\n",
    "                                 add_special_tokens=True,  # add [CLS], [SEP]\n",
    "                                 max_length=max_length,  # max length of the text that can go to RoBERTa\n",
    "                                 truncation=True,\n",
    "                                 padding= 'max_length',  # add [PAD] tokens at the end of sentence\n",
    "                                 return_attention_mask=True,  # add attention mask to not focus on pad tokens\n",
    "                                 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "# map to the expected input to TFRobertaForSequenceClassification, see here\n",
    "def map_example_to_dict(input_ids, attention_masks, label):\n",
    "    return {\n",
    "      \"input_ids\": input_ids,\n",
    "      \"attention_mask\": attention_masks,\n",
    "           }, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_examples(ds, limit=-1):\n",
    "    # Prepare Input list\n",
    "    input_ids_list = []\n",
    "    attention_mask_list = []\n",
    "    label_list = []\n",
    "\n",
    "    if (limit > 0):\n",
    "        ds = ds.take(limit)\n",
    "\n",
    "    for review, label in tfds.as_numpy(ds):\n",
    "        bert_input = convert_example_to_feature(review.decode())\n",
    "        input_ids_list.append(bert_input['input_ids'])\n",
    "        attention_mask_list.append(bert_input['attention_mask'])\n",
    "        label_list.append([label])\n",
    "\n",
    "    return tf.data.Dataset.from_tensor_slices((input_ids_list,\n",
    "                                               attention_mask_list,\n",
    "                                               label_list)).map(map_example_to_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_sentences_modified = tf.data.Dataset.from_tensor_slices((sentence_train,\n",
    "                                                                  y_train))\n",
    "\n",
    "val_sentences_modified = tf.data.Dataset.from_tensor_slices((sentence_val,\n",
    "                                                                 y_val))\n",
    "\n",
    "testing_sentences_modified = tf.data.Dataset.from_tensor_slices((sentence_test,\n",
    "                                                                 y_test))\n",
    "\n",
    "green_testing_sentences_modified = tf.data.Dataset.from_tensor_slices((greenenergytest_clean['text'],\n",
    "                                                                 greenenergytest_clean['polarity']))\n",
    "\n",
    "ds_train_encoded = encode_examples(training_sentences_modified).shuffle(10000).batch(batch_size)\n",
    "ds_val_encoded = encode_examples(val_sentences_modified).batch(batch_size)\n",
    "ds_test_encoded = encode_examples(testing_sentences_modified).batch(batch_size)\n",
    "ds_green_encoded = encode_examples(green_testing_sentences_modified).batch(batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 1e-05\n",
    "number_of_epochs = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at distilroberta-base were not used when initializing TFRobertaForSequenceClassification: ['lm_head']\n",
      "- This IS expected if you are initializing TFRobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFRobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some layers of TFRobertaForSequenceClassification were not initialized from the model checkpoint at distilroberta-base and are newly initialized: ['classifier']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = TFRobertaForSequenceClassification.from_pretrained(\"distilroberta-base\",return_dict=True)\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate, epsilon=1e-08,beta_1=0.9)\n",
    "\n",
    "# we do not have one-hot vectors, we can use sparce categorical cross entropy and accuracy\n",
    "loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "metric = tf.keras.metrics.SparseCategoricalAccuracy('accuracy')\n",
    "model.compile(optimizer=optimizer, loss=loss, metrics=[metric])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/8\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_for_sequence_classification_6/roberta/pooler/dense/kernel:0', 'tf_roberta_for_sequence_classification_6/roberta/pooler/dense/bias:0'] when minimizing the loss.\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_for_sequence_classification_6/roberta/pooler/dense/kernel:0', 'tf_roberta_for_sequence_classification_6/roberta/pooler/dense/bias:0'] when minimizing the loss.\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_for_sequence_classification_6/roberta/pooler/dense/kernel:0', 'tf_roberta_for_sequence_classification_6/roberta/pooler/dense/bias:0'] when minimizing the loss.\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_for_sequence_classification_6/roberta/pooler/dense/kernel:0', 'tf_roberta_for_sequence_classification_6/roberta/pooler/dense/bias:0'] when minimizing the loss.\n",
      "426/426 [==============================] - 40s 94ms/step - loss: 0.3671 - accuracy: 0.8353 - val_loss: 0.2705 - val_accuracy: 0.8885\n",
      "Epoch 2/8\n",
      "426/426 [==============================] - 40s 93ms/step - loss: 0.2636 - accuracy: 0.8969 - val_loss: 0.2549 - val_accuracy: 0.8957\n",
      "Epoch 3/8\n",
      "426/426 [==============================] - 40s 93ms/step - loss: 0.2256 - accuracy: 0.9135 - val_loss: 0.2606 - val_accuracy: 0.8961\n",
      "Epoch 4/8\n",
      "426/426 [==============================] - 40s 93ms/step - loss: 0.1938 - accuracy: 0.9271 - val_loss: 0.2750 - val_accuracy: 0.8945\n",
      "Epoch 5/8\n",
      "426/426 [==============================] - 40s 94ms/step - loss: 0.1616 - accuracy: 0.9424 - val_loss: 0.2874 - val_accuracy: 0.8991\n",
      "Epoch 6/8\n",
      "426/426 [==============================] - 40s 94ms/step - loss: 0.1344 - accuracy: 0.9508 - val_loss: 0.3089 - val_accuracy: 0.8961\n",
      "Epoch 7/8\n",
      "426/426 [==============================] - 40s 94ms/step - loss: 0.1113 - accuracy: 0.9627 - val_loss: 0.2999 - val_accuracy: 0.8976\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "es = EarlyStopping(patience=2,restore_best_weights=True,monitor='val_accuracy')\n",
    "history = model.fit(ds_train_encoded, epochs=number_of_epochs,\n",
    "          validation_data=ds_val_encoded, callbacks=[es])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from tensorflow.keras import layers, models\n",
    "\n",
    "#def add_last_layers(model):\n",
    "#    base_model = set_nontrainable_layers(model)\n",
    "#    flatten_layer = layers.Flatten()\n",
    "#    dense_layer = layers.Dense(500, activation='relu')\n",
    "#    prediction_layer = layers.Dense(3, activation='softmax')\n",
    "    \n",
    "    \n",
    "#    model = models.Sequential([\n",
    "#        model,\n",
    "#        flatten_layer,\n",
    "#        dense_layer,\n",
    "#        prediction_layer\n",
    "#    ])\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 1., 0., ..., 0., 0., 0.], dtype=float32)"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "pred = model.predict(ds_test_encoded)\n",
    "logit = pred.logits\n",
    "prediction = tf.round(tf.nn.sigmoid(logit))\n",
    "pred = prediction[:,1].numpy()\n",
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_loss(history):\n",
    "    plt.plot(history.history['loss'])\n",
    "    plt.plot(history.history['val_loss'])\n",
    "    plt.title('Model loss')\n",
    "    plt.ylabel('Mean Square Error - Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend(['Train', 'Validation'], loc='best')\n",
    "    plt.show()\n",
    "    \n",
    "    plt.plot(history.history['accuracy'])\n",
    "    plt.plot(history.history['val_accuracy'])\n",
    "    plt.title('Model Accuracy')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend(['Train', 'Validation'], loc='best')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_loss(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "68/68 [==============================] - 2s 29ms/step - loss: 0.2923 - accuracy: 0.9005\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.2923433780670166, 0.9004629850387573]"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(ds_test_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7/7 [==============================] - 0s 24ms/step - loss: 0.8474 - accuracy: 0.7204\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.8473857641220093, 0.7203791737556458]"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(ds_green_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import models\n",
    "model.save_pretrained('../models/model_roBERTa_test_distil')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/hamish/.virtualenvs/henry2/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This property should not be used in TensorFlow 2.0, as updates are applied automatically.\n",
      "WARNING:tensorflow:From /home/hamish/.virtualenvs/henry2/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This property should not be used in TensorFlow 2.0, as updates are applied automatically.\n",
      "INFO:tensorflow:Assets written to: model_roBERTa_binary_3/assets\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from tensorflow.keras import models\n",
    "\n",
    "# Let's say that you have a `model`\n",
    "# You can save it :\n",
    "\n",
    "models.save_model(model, 'model_roBERTa_binary_3')\n",
    "# and you can load it somewhere else :\n",
    "#loaded_model = models.load_model('my_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import models\n",
    "loaded_model = models.load_model('../models/model_roBERTa_binary_2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'TFRobertaForSequenceClassification' object has no attribute 'state_dict'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-40-c357a236bece>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'../models/model_roBERTa_binary_4_dict'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mloaded_model_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'../models/model_roBERTa_binary_4_dict'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'TFRobertaForSequenceClassification' object has no attribute 'state_dict'"
     ]
    }
   ],
   "source": [
    "models.save_model(model.state_dict(), '../models/model_roBERTa_binary_4_dict')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuned_model = TFRobertaForSequenceClassification.from_pretrained(\"roberta-base\",num_labels=NCLASSES,output_attentions=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuned_model.load_state_dict(tf.load('../models/model_roBERTa_binary_4_dict'))\n",
    "#tuned_model.load_state_dict(tf.load(’/content/drive/My Drive/ftmodelname’,\n",
    "map_location=torch.device(“cpu”)),strict=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"tf_roberta_for_sequence_classification_11\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "roberta (TFRobertaMainLayer) multiple                  124645632 \n",
      "_________________________________________________________________\n",
      "classifier (TFRobertaClassif multiple                  592130    \n",
      "=================================================================\n",
      "Total params: 125,237,762\n",
      "Trainable params: 125,237,762\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "loaded_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "signature_wrapper(input_ids) got unexpected keyword arguments: attention_mask.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-38-4e4f8d2519e5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0minference_func\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloaded_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msignatures\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"serving_default\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mds_test_encoded\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minference_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpand_dims\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.virtualenvs/henry2/lib/python3.6/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1653\u001b[0m       \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIf\u001b[0m \u001b[0mthe\u001b[0m \u001b[0marguments\u001b[0m \u001b[0mdo\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mmatch\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mfunction\u001b[0m\u001b[0;31m'\u001b[0m\u001b[0ms\u001b[0m \u001b[0msignature\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1654\u001b[0m     \"\"\"\n\u001b[0;32m-> 1655\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1656\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1657\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/henry2/lib/python3.6/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, args, kwargs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1671\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mstructured_err\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1672\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1673\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_with_flat_signature\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1674\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1675\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_with_flat_signature\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/henry2/lib/python3.6/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_with_flat_signature\u001b[0;34m(self, args, kwargs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1712\u001b[0m               self._flat_signature_summary(), unused_key))\n\u001b[1;32m   1713\u001b[0m       raise TypeError(\"{} got unexpected keyword arguments: {}.\".format(\n\u001b[0;32m-> 1714\u001b[0;31m           self._flat_signature_summary(), \", \".join(sorted(kwargs))))\n\u001b[0m\u001b[1;32m   1715\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1716\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: signature_wrapper(input_ids) got unexpected keyword arguments: attention_mask."
     ]
    }
   ],
   "source": [
    "inference_func = loaded_model.signatures[\"serving_default\"]\n",
    "for inputs,_ in ds_test_encoded:\n",
    "  print(inference_func(**({k: tf.expand_dims(v, axis=0) for k, v in inputs.items()})))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inference_func(**({k: tf.expand_dims(v, axis=0) for k, v in inputs.items()}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class ModelMetrics(tf.keras.callbacks.Callback):\n",
    "#     def on_train_begin(self, logs={}):\n",
    "#         self.count_n = 1\n",
    "\n",
    "#     def on_epoch_end(self, batch, logs={}):\n",
    "        \n",
    "#         os.mkdir('/create/directory/for/model/' + str(self.count_n))\n",
    "#         self.model.save_pretrained('/save/trained/model/here/' + str(self.count_n)) # this folder address should match with folder we created above\n",
    "        \n",
    "#         y_val_pred = tf.nn.softmax(self.model.predict(ds_test_encoded))\n",
    "#         y_pred_argmax = tf.math.argmax(y_val_pred, axis=1)\n",
    "#         testing_copy = testing_sentences.copy()\n",
    "#         testing_copy['predicted'] = y_pred_argmax\n",
    "#         f1_s = f1_score(testing_sentences['label'], testing_copy['predicted'])\n",
    "#         print('\\n f1 score is :', f1_s)\n",
    "#         self.count_n += 1\n",
    "\n",
    "# metrics = ModelMetrics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: You are using pip version 20.1.1; however, version 20.2.4 is available.\r\n",
      "You should consider upgrading via the '/home/hamish/.virtualenvs/henry2/bin/python -m pip install --upgrade pip' command.\u001b[0m\r\n"
     ]
    }
   ],
   "source": [
    "!pip install -q -U keras-tuner\n",
    "import kerastuner as kt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_builder(hp):\n",
    "    hp_learning_rate = hp.Choice('learning_rate', values = [7e-5, 1e-5, 12e-5])\n",
    "    hp_epsilon = hp.Choice('epsilon', values = [1e-08, 1e-07, 1e-06])\n",
    "    hp_beta_1 = hp.Choice('beta_1', values = [0.91, 0.9, 0.85])\n",
    "    hp_decay = hp.Choice('weight_decay', values = [0., 0.001, 0.0015])\n",
    "    model = TFRobertaForSequenceClassification.from_pretrained(\"roberta-base\")\n",
    "    optimizer = AdamWeightDecay(learning_rate= hp_learning_rate, epsilon=hp_epsilon,beta_1=hp_beta_1,weight_decay_rate=hp_decay)\n",
    "\n",
    "    # we do not have one-hot vectors, we can use sparce categorical cross entropy and accuracy\n",
    "    loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "    metric = tf.keras.metrics.SparseCategoricalAccuracy('accuracy')\n",
    "    model.compile(optimizer=optimizer, loss=loss, metrics=[metric])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Reloading Oracle from existing project my_dir/roberta-tuning/oracle.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at roberta-base were not used when initializing TFRobertaForSequenceClassification: ['lm_head']\n",
      "- This IS expected if you are initializing TFRobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFRobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some layers of TFRobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Reloading Tuner from my_dir/roberta-tuning/tuner0.json\n"
     ]
    }
   ],
   "source": [
    "tuner = kt.Hyperband(model_builder,\n",
    "                     objective = 'val_accuracy', \n",
    "                     max_epochs = 10,\n",
    "                     factor = 3,\n",
    "                     directory = 'my_dir',\n",
    "                     project_name = 'roberta-tuning')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import IPython\n",
    "class ClearTrainingOutput(tf.keras.callbacks.Callback):\n",
    "      def on_train_end(*args, **kwargs):\n",
    "        IPython.display.clear_output(wait = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 12 Complete [00h 07m 50s]\n",
      "val_accuracy: 0.8970840573310852\n",
      "\n",
      "Best val_accuracy So Far: 0.9025728702545166\n",
      "Total elapsed time: 01h 00m 16s\n",
      "INFO:tensorflow:Oracle triggered exit\n",
      "\n",
      "The hyperparameter search is complete. The optimal \n",
      "learning rate is 1e-05 and epsilon 1e-08 and beta 0.9 and decay 0.001 .\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "es = EarlyStopping(patience=3,restore_best_weights=True,monitor='val_accuracy')\n",
    "tuner.search(ds_train_encoded, epochs = 10, validation_data = ds_val_encoded, callbacks = [es,ClearTrainingOutput()])\n",
    "\n",
    "# Get the optimal hyperparameters\n",
    "best_hps = tuner.get_best_hyperparameters(num_trials = 1)[0]\n",
    "\n",
    "print(f\"\"\"\n",
    "The hyperparameter search is complete. The optimal \n",
    "learning rate is {best_hps.get('learning_rate')} and epsilon {best_hps.get('epsilon')} and beta {best_hps.get('beta_1')} and decay {best_hps.get('weight_decay')} .\n",
    "\"\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at roberta-base were not used when initializing TFRobertaForSequenceClassification: ['lm_head']\n",
      "- This IS expected if you are initializing TFRobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFRobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some layers of TFRobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_for_sequence_classification/roberta/pooler/dense/kernel:0', 'tf_roberta_for_sequence_classification/roberta/pooler/dense/bias:0'] when minimizing the loss.\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_for_sequence_classification/roberta/pooler/dense/kernel:0', 'tf_roberta_for_sequence_classification/roberta/pooler/dense/bias:0'] when minimizing the loss.\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_for_sequence_classification/roberta/pooler/dense/kernel:0', 'tf_roberta_for_sequence_classification/roberta/pooler/dense/bias:0'] when minimizing the loss.\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_for_sequence_classification/roberta/pooler/dense/kernel:0', 'tf_roberta_for_sequence_classification/roberta/pooler/dense/bias:0'] when minimizing the loss.\n",
      "107/107 [==============================] - 58s 540ms/step - loss: 0.4207 - accuracy: 0.8041 - val_loss: 0.2755 - val_accuracy: 0.8933\n",
      "Epoch 2/10\n",
      "107/107 [==============================] - 56s 526ms/step - loss: 0.2640 - accuracy: 0.8938 - val_loss: 0.2582 - val_accuracy: 0.8979\n",
      "Epoch 3/10\n",
      "107/107 [==============================] - 57s 529ms/step - loss: 0.2306 - accuracy: 0.9105 - val_loss: 0.2877 - val_accuracy: 0.8983\n",
      "Epoch 4/10\n",
      "107/107 [==============================] - 57s 529ms/step - loss: 0.2057 - accuracy: 0.9219 - val_loss: 0.2788 - val_accuracy: 0.8979\n",
      "Epoch 5/10\n",
      "107/107 [==============================] - 57s 529ms/step - loss: 0.1773 - accuracy: 0.9335 - val_loss: 0.2698 - val_accuracy: 0.9034\n",
      "Epoch 6/10\n",
      "107/107 [==============================] - 56s 528ms/step - loss: 0.1622 - accuracy: 0.9420 - val_loss: 0.2901 - val_accuracy: 0.9017\n",
      "Epoch 7/10\n",
      "107/107 [==============================] - 56s 528ms/step - loss: 0.1339 - accuracy: 0.9552 - val_loss: 0.2837 - val_accuracy: 0.9021\n",
      "Epoch 8/10\n",
      "107/107 [==============================] - 56s 526ms/step - loss: 0.1198 - accuracy: 0.9574 - val_loss: 0.3283 - val_accuracy: 0.8986\n",
      "Epoch 9/10\n",
      "107/107 [==============================] - 56s 526ms/step - loss: 0.1030 - accuracy: 0.9651 - val_loss: 0.3346 - val_accuracy: 0.9022\n",
      "Epoch 10/10\n",
      "107/107 [==============================] - 56s 525ms/step - loss: 0.0867 - accuracy: 0.9716 - val_loss: 0.3651 - val_accuracy: 0.9007\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f0aca83ff98>"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = tuner.hypermodel.build(best_hps)\n",
    "model.fit(ds_train_encoded, epochs = 10, validation_data = ds_val_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17/17 [==============================] - 3s 151ms/step - loss: 0.3372 - accuracy: 0.9009\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.3371899425983429, 0.9009259343147278]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(ds_test_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/hamish/.virtualenvs/henry2/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This property should not be used in TensorFlow 2.0, as updates are applied automatically.\n",
      "WARNING:tensorflow:From /home/hamish/.virtualenvs/henry2/lib/python3.6/site-packages/tensorflow/python/training/tracking/tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This property should not be used in TensorFlow 2.0, as updates are applied automatically.\n",
      "INFO:tensorflow:Assets written to: ../raw_data/models/model_roBERTa_lr1e-05epsilon1e-08beta0.9decay0.001/assets\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras import models\n",
    "models.save_model(model, '../raw_data/models/model_roBERTa_lr1e-05epsilon1e-08beta0.9decay0.001')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Trial 30 Complete [00h 03m 39s]\n",
    "val_accuracy: 0.9009922742843628\n",
    "\n",
    "Best val_accuracy So Far: 0.9023153185844421\n",
    "Total elapsed time: 00h 59m 41s\n",
    "INFO:tensorflow:Oracle triggered exit\n",
    "\n",
    "The hyperparameter search is complete. The optimal \n",
    "learning rate is 1e-05 and epsilon 1e-06 and beta 0.9 and decay 0.001 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "henry2",
   "language": "python",
   "name": "henry2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
