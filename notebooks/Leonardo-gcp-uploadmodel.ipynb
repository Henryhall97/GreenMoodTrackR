{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '../')\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_emb_binary.joblib model_emb_binary.sav    model_w2v_binary.joblib\r\n"
     ]
    }
   ],
   "source": [
    "! ls ../models/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Model, and Upload (without removing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from green_mood_tracker.data import get_data, clean\n",
    "from green_mood_tracker.roberta_trainer import RobertaTrainer\n",
    "from termcolor import colored"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "############   Loading Data   ############\n",
      "shape: (100,)\n",
      "size: 0.00088 Mb\n",
      "\u001b[31m############  Training model   ############\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at roberta-base were not used when initializing TFRobertaForSequenceClassification: ['lm_head']\n",
      "- This IS expected if you are initializing TFRobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFRobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some layers of TFRobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_for_sequence_classification_1/roberta/pooler/dense/kernel:0', 'tf_roberta_for_sequence_classification_1/roberta/pooler/dense/bias:0'] when minimizing the loss.\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_for_sequence_classification_1/roberta/pooler/dense/kernel:0', 'tf_roberta_for_sequence_classification_1/roberta/pooler/dense/bias:0'] when minimizing the loss.\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_for_sequence_classification_1/roberta/pooler/dense/kernel:0', 'tf_roberta_for_sequence_classification_1/roberta/pooler/dense/bias:0'] when minimizing the loss.\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_for_sequence_classification_1/roberta/pooler/dense/kernel:0', 'tf_roberta_for_sequence_classification_1/roberta/pooler/dense/bias:0'] when minimizing the loss.\n",
      "2/2 [==============================] - 21s 11s/step - loss: 0.7267 - accuracy: 0.4082 - val_loss: 0.5722 - val_accuracy: 0.8571\n",
      "Epoch 2/3\n",
      "2/2 [==============================] - 15s 7s/step - loss: 0.6021 - accuracy: 0.7347 - val_loss: 0.4183 - val_accuracy: 0.8571\n",
      "Epoch 3/3\n",
      "2/2 [==============================] - 14s 7s/step - loss: 0.5631 - accuracy: 0.7347 - val_loss: 0.4404 - val_accuracy: 0.8571\n",
      "train 309.3\n",
      "\u001b[34m############  Evaluating model ############\u001b[0m\n",
      "2/2 [==============================] - 3s 1s/step - loss: 0.5294 - accuracy: 0.7347\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.5261 - accuracy: 0.7667\n",
      "\u001b[34maccuracy train: 0.7346938848495483 || accuracy test: 0.7666666507720947\u001b[0m\n",
      "\u001b[32m############   Saving model    ############\u001b[0m\n",
      "\u001b[32mroBERTa.tf saved locally\u001b[0m\n",
      "\u001b[32m=> roBERTa.tf uploaded to bucket green-mood-tracker-01 inside models/RoBERTa/v0/roBERTa.tf\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "# Get and clean data\n",
    "EXPERIMENT = \"[GB] [London] [green_mood_tracker] RoBERTa\"\n",
    "\n",
    "params = dict(nrows=100,\n",
    "              upload=True,\n",
    "              local=False,\n",
    "              rm=False,\n",
    "              mlflow=True,  # set to True to log params to mlflow\n",
    "              experiment_name=EXPERIMENT\n",
    "              )\n",
    "\n",
    "print(\"############   Loading Data   ############\")\n",
    "df = get_data(**params)\n",
    "df = clean(df, 'text')\n",
    "y_train = df.polarity\n",
    "X_train = df.text\n",
    "del df\n",
    "print(\"shape: {}\".format(X_train.shape))\n",
    "print(\"size: {} Mb\".format(X_train.memory_usage() / 1e6))\n",
    "# Train and save model, locally and\n",
    "t = RobertaTrainer(X=X_train, y=y_train, **params)\n",
    "del X_train, y_train\n",
    "print(colored(\"############  Training model   ############\", \"red\"))\n",
    "t.train()\n",
    "print(colored(\"############  Evaluating model ############\", \"blue\"))\n",
    "t.evaluate()\n",
    "print(colored(\"############   Saving model    ############\", \"green\"))\n",
    "t.save_model(**params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Upload the Model (Only)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from green_mood_tracker.gcp import storage_upload_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploading roBERTa.tf!\n",
      "\u001b[32m=> roBERTa.tf uploaded to bucket green-mood-tracker-01 inside models/RoBERTa/test/roBERTa.tf\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "storage_upload_models(bucket_name='green-mood-tracker-01', model_name='RoBERTa',\n",
    "                      model_version='test', model_filename='roBERTa.tf', rm=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download the Model from GCP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
