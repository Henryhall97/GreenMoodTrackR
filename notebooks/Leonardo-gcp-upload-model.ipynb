{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/leonardogavaudan/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/leonardogavaudan/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/leonardogavaudan/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '../')\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_emb_binary.joblib model_emb_binary.sav    model_w2v_binary.joblib\r\n"
     ]
    }
   ],
   "source": [
    "! ls ../models/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Model Locally (without removing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from green_mood_tracker.data import get_data, clean\n",
    "from green_mood_tracker.roberta_trainer import RobertaTrainer\n",
    "from termcolor import colored"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "############   Loading Data   ############\n",
      "shape: (100,)\n",
      "size: 0.00088 Mb\n",
      "\u001b[31m############  Training model   ############\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some layers from the model checkpoint at roberta-base were not used when initializing TFRobertaForSequenceClassification: ['lm_head']\n",
      "- This IS expected if you are initializing TFRobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFRobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some layers of TFRobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_for_sequence_classification_2/roberta/pooler/dense/kernel:0', 'tf_roberta_for_sequence_classification_2/roberta/pooler/dense/bias:0'] when minimizing the loss.\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_for_sequence_classification_2/roberta/pooler/dense/kernel:0', 'tf_roberta_for_sequence_classification_2/roberta/pooler/dense/bias:0'] when minimizing the loss.\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_for_sequence_classification_2/roberta/pooler/dense/kernel:0', 'tf_roberta_for_sequence_classification_2/roberta/pooler/dense/bias:0'] when minimizing the loss.\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_roberta_for_sequence_classification_2/roberta/pooler/dense/kernel:0', 'tf_roberta_for_sequence_classification_2/roberta/pooler/dense/bias:0'] when minimizing the loss.\n",
      "2/2 [==============================] - 26s 13s/step - loss: 0.6351 - accuracy: 0.7347 - val_loss: 0.5400 - val_accuracy: 0.7700\n",
      "train 123.22\n",
      "\u001b[34m############  Evaluating model ############\u001b[0m\n",
      "2/2 [==============================] - 2s 926ms/step - loss: 0.5625 - accuracy: 0.7347\n",
      "3/3 [==============================] - 5s 2s/step - loss: 0.5575 - accuracy: 0.7468\n",
      "\u001b[34maccuracy train: 0.7346938848495483 || accuracy test: 0.746835470199585\u001b[0m\n",
      "\u001b[32m############   Saving model    ############\u001b[0m\n",
      "\u001b[32mroBERTa.tf saved locally\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "# Get and clean data\n",
    "EXPERIMENT = \"[GB] [London] [green_mood_tracker] RoBERTa\"\n",
    "\n",
    "params = dict(nrows=100,\n",
    "              upload=False,\n",
    "              local=False,\n",
    "              rm=False,\n",
    "              mlflow=True,  # set to True to log params to mlflow\n",
    "              experiment_name=EXPERIMENT\n",
    "              )\n",
    "\n",
    "print(\"############   Loading Data   ############\")\n",
    "df = get_data(**params)\n",
    "df = clean(df, 'text')\n",
    "y_train = df.polarity\n",
    "X_train = df.text\n",
    "del df\n",
    "print(\"shape: {}\".format(X_train.shape))\n",
    "print(\"size: {} Mb\".format(X_train.memory_usage() / 1e6))\n",
    "# Train and save model, locally and\n",
    "t = RobertaTrainer(X=X_train, y=y_train, **params)\n",
    "del X_train, y_train\n",
    "print(colored(\"############  Training model   ############\", \"red\"))\n",
    "t.train()\n",
    "print(colored(\"############  Evaluating model ############\", \"blue\"))\n",
    "t.evaluate()\n",
    "print(colored(\"############   Saving model    ############\", \"green\"))\n",
    "t.save_model(**params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Model on GCP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Upload the Model (without removing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "from green_mood_tracker.gcp import storage_upload_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploading roBERTa.tf!\n",
      "\u001b[32m=> roBERTa.tf uploaded to bucket green-mood-tracker-01 inside models/RoBERTa/v0\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "storage_upload_models(bucket_name='green-mood-tracker-01', model_name='RoBERTa',\n",
    "                      model_version='v0', model_filename='roBERTa.tf', rm=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download the Model from GCP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "from green_mood_tracker.gcp import download_model_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m=> roBERTa.tf downloaded from storage\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "download_model_files(bucket_name='green-mood-tracker-01', model_name='RoBERTa',\n",
    "                     model_version='test', model_filename='roBERTa.tf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from green_mood_tracker.gcp import load_model\n",
    "from predict import generate_prediction, get_test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
      "\n",
      "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at models/roBERTa.tf.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m=> loaded model roBERTa.tf\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "model = load_model(model_name='RoBERTa', model_filename='roBERTa.tf', rm=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = get_test_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFRobertaForSequenceClassification.\n",
      "\n",
      "All the layers of TFRobertaForSequenceClassification were initialized from the model checkpoint at models/roBERTa.tf.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m=> loaded model roBERTa.tf\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0.28979734, 0.30191797, 0.29278377, 0.285399  , 0.29739454,\n",
       "       0.29063857, 0.29380593, 0.2921131 , 0.28735808, 0.2986038 ,\n",
       "       0.28978133, 0.29879144, 0.28313825, 0.2995036 , 0.30711553,\n",
       "       0.30123916, 0.30399767, 0.3074397 , 0.29629147, 0.2955789 ,\n",
       "       0.31266344, 0.29394478, 0.2926669 , 0.2871198 , 0.3176377 ,\n",
       "       0.29562467, 0.3011247 , 0.30538484, 0.29414368, 0.27889526,\n",
       "       0.28754598, 0.30617523, 0.31275502, 0.30208433, 0.311403  ,\n",
       "       0.30160436, 0.30092445, 0.3151674 , 0.29466775, 0.29442784,\n",
       "       0.30799472, 0.2906482 , 0.289969  , 0.30760434, 0.30768386,\n",
       "       0.29333088, 0.2889149 , 0.3040571 , 0.29423466, 0.29803804,\n",
       "       0.28599605, 0.28270513, 0.2977698 , 0.29553753, 0.30228886,\n",
       "       0.28845158, 0.3013547 , 0.28848734, 0.2914635 , 0.29367706,\n",
       "       0.29724872, 0.3052055 , 0.28994074, 0.27883455, 0.2863143 ,\n",
       "       0.2826571 , 0.305679  , 0.27918622, 0.3002725 , 0.2986722 ,\n",
       "       0.28175053, 0.29999682, 0.30123734, 0.29506105, 0.29157302,\n",
       "       0.29574853, 0.29850057, 0.29505327, 0.2935873 , 0.28435636,\n",
       "       0.28000265, 0.28332192, 0.29864538, 0.303061  , 0.27958605,\n",
       "       0.30204082, 0.30594838, 0.31024158, 0.28886232, 0.30422434,\n",
       "       0.28021577, 0.29637283, 0.2899082 , 0.30296934, 0.30863863,\n",
       "       0.29550627, 0.28255418, 0.29727247, 0.28977627, 0.29717252,\n",
       "       0.2881918 , 0.28877154, 0.28566363, 0.3103989 , 0.31449163,\n",
       "       0.29048353, 0.2944809 , 0.29569265, 0.3004735 , 0.29536515,\n",
       "       0.29793683, 0.29294798, 0.29678214, 0.2886592 , 0.29332522,\n",
       "       0.28369883, 0.29616588, 0.30342475, 0.2866635 , 0.28562176,\n",
       "       0.31373158, 0.306672  , 0.29145357, 0.3011773 , 0.2755464 ,\n",
       "       0.30069378, 0.29059553, 0.29138532, 0.2960362 , 0.31166258,\n",
       "       0.2901979 , 0.29803586, 0.2907578 , 0.2998444 , 0.2934831 ,\n",
       "       0.29207438, 0.28292844, 0.30677947, 0.29487363, 0.2981395 ,\n",
       "       0.30252117, 0.2911209 , 0.2782588 , 0.2764599 , 0.2948643 ,\n",
       "       0.2924722 , 0.28552324, 0.3172797 , 0.30217385, 0.29667166,\n",
       "       0.2927404 , 0.26449254, 0.3154561 , 0.28878075, 0.30398482,\n",
       "       0.29775396, 0.2881948 , 0.2850029 , 0.3075046 , 0.2905188 ,\n",
       "       0.304738  , 0.30202183, 0.3010706 , 0.3166199 , 0.2705516 ,\n",
       "       0.29212183, 0.30701816, 0.28257242, 0.28752983, 0.29932448,\n",
       "       0.2860557 , 0.2998422 , 0.2998344 , 0.29318908, 0.30956128,\n",
       "       0.30060747, 0.30311683, 0.30454636, 0.3138531 , 0.28729695,\n",
       "       0.3020218 , 0.28728956, 0.285578  , 0.285578  , 0.285578  ,\n",
       "       0.285578  , 0.285578  , 0.29117426, 0.29520142, 0.29020715,\n",
       "       0.311315  , 0.29870096, 0.29275316, 0.3062309 , 0.30572933,\n",
       "       0.30070743, 0.29977298, 0.29602683, 0.29614666, 0.28288996,\n",
       "       0.30084518, 0.30618927, 0.29743978, 0.29002818, 0.31058815,\n",
       "       0.29692936, 0.32064825, 0.2808095 , 0.2998148 , 0.2858692 ,\n",
       "       0.27862275], dtype=float32)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_prediction(data, download_files=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
