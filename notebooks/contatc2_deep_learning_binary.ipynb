{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\cleme\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "from green_mood_tracker.training_data import get_raw_data_notebook\n",
    "from green_mood_tracker.data_cleaning import clean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## download and clean data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\cleme\\.venvs\\lewagon\\lib\\site-packages\\green_mood_tracker\\training_data.py:24: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  sentiment140_final['source'] = 'sentiment140'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 21.3 s\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>polarity</th>\n",
       "      <th>source</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1467933112</td>\n",
       "      <td>the angel is going to miss the athlete this we...</td>\n",
       "      <td>0</td>\n",
       "      <td>sts_gold</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2323395086</td>\n",
       "      <td>It looks as though Shaq is getting traded to C...</td>\n",
       "      <td>0</td>\n",
       "      <td>sts_gold</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1467968979</td>\n",
       "      <td>@clarianne APRIL 9TH ISN'T COMING SOON ENOUGH</td>\n",
       "      <td>0</td>\n",
       "      <td>sts_gold</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1990283756</td>\n",
       "      <td>drinking a McDonalds coffee and not understand...</td>\n",
       "      <td>0</td>\n",
       "      <td>sts_gold</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1988884918</td>\n",
       "      <td>So dissapointed Taylor Swift doesnt have a Twi...</td>\n",
       "      <td>0</td>\n",
       "      <td>sts_gold</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           id                                               text  polarity  \\\n",
       "0  1467933112  the angel is going to miss the athlete this we...         0   \n",
       "1  2323395086  It looks as though Shaq is getting traded to C...         0   \n",
       "2  1467968979     @clarianne APRIL 9TH ISN'T COMING SOON ENOUGH          0   \n",
       "3  1990283756  drinking a McDonalds coffee and not understand...         0   \n",
       "4  1988884918  So dissapointed Taylor Swift doesnt have a Twi...         0   \n",
       "\n",
       "     source  \n",
       "0  sts_gold  \n",
       "1  sts_gold  \n",
       "2  sts_gold  \n",
       "3  sts_gold  \n",
       "4  sts_gold  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "raw_data = get_raw_data_notebook(True)\n",
    "raw_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1633048, 4)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sentiment140              1600000\n",
       "kaggle_sentiment_train      27480\n",
       "kaggle_sentiment_test        3534\n",
       "sts_gold                     2034\n",
       "Name: source, dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_data.source.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2    810317\n",
       "0    810184\n",
       "1     12547\n",
       "Name: polarity, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_data.polarity.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## sample data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_sample = raw_data.sample(n=25_000,random_state=0).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 23.1 s\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>polarity</th>\n",
       "      <th>source</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>89497</td>\n",
       "      <td>1755343696</td>\n",
       "      <td>headed yonkers back later tonight</td>\n",
       "      <td>0</td>\n",
       "      <td>sentiment140</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>212595</td>\n",
       "      <td>1974776713</td>\n",
       "      <td>tried callin past dayz tu e bueno wey</td>\n",
       "      <td>0</td>\n",
       "      <td>sentiment140</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>17473</td>\n",
       "      <td>1556303397</td>\n",
       "      <td>sister give much shit twitter im uncool</td>\n",
       "      <td>0</td>\n",
       "      <td>sentiment140</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1526772</td>\n",
       "      <td>2177082321</td>\n",
       "      <td>thanks sun shining manchester today</td>\n",
       "      <td>2</td>\n",
       "      <td>sentiment140</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1481406</td>\n",
       "      <td>2067068036</td>\n",
       "      <td>saw stand lookin good</td>\n",
       "      <td>2</td>\n",
       "      <td>sentiment140</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     index          id                                     text  polarity  \\\n",
       "0    89497  1755343696        headed yonkers back later tonight         0   \n",
       "1   212595  1974776713    tried callin past dayz tu e bueno wey         0   \n",
       "2    17473  1556303397  sister give much shit twitter im uncool         0   \n",
       "3  1526772  2177082321      thanks sun shining manchester today         2   \n",
       "4  1481406  2067068036                    saw stand lookin good         2   \n",
       "\n",
       "         source  \n",
       "0  sentiment140  \n",
       "1  sentiment140  \n",
       "2  sentiment140  \n",
       "3  sentiment140  \n",
       "4  sentiment140  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "data_sample_clean = clean(data_sample,'text')\n",
    "data_sample_clean.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## split data Train/Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = data_sample_clean.text\n",
    "y = data_sample_clean.polarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# move to categorical if we're not using binary data\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "y = to_categorical(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_train, sentence_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "\n",
    "tk = Tokenizer()\n",
    "tk.fit_on_texts(sentence_train)\n",
    "word_to_id = tk.word_index\n",
    "id_to_word = {value:key for key,value in word_to_id.items()}\n",
    "sentences_token_train = tk.texts_to_sequences(sentence_train)\n",
    "sentences_token_test = tk.texts_to_sequences(sentence_test)\n",
    "\n",
    "vocab_size=len(word_to_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## pad data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_data(X_train,X_test):\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    X_train_pad = pad_sequences(X_train, dtype = 'float32',padding='post')\n",
    "    X__test_pad = pad_sequences(X_test, dtype = 'float32',padding='post')\n",
    "    \n",
    "    return X_train_pad,X__test_pad\n",
    "\n",
    "X_train_pad, X_test_pad = generate_data(sentences_token_train,sentences_token_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_loss(history):\n",
    "    plt.plot(history.history['loss'])\n",
    "    plt.plot(history.history['val_loss'])\n",
    "    plt.title('Model loss')\n",
    "    plt.ylabel('Mean Square Error - Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend(['Train', 'Validation'], loc='best')\n",
    "    plt.show()\n",
    "    \n",
    "    plt.plot(history.history['accuracy'])\n",
    "    plt.plot(history.history['val_accuracy'])\n",
    "    plt.title('Model Accuracy')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend(['Train', 'Validation'], loc='best')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## build lstm model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_model(vocab_size):\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    model = Sequential()\n",
    "    model.add(layers.Embedding(input_dim=vocab_size+1, output_dim=100, mask_zero=True))\n",
    "    model.add(layers.GRU(units=13, activation='tanh')) \n",
    "    model.add(layers.Dense(64, activation='relu'))\n",
    "    model.add(layers.Dropout(.2))\n",
    "    model.add(layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "    model.compile(loss= 'binary_crossentropy', \n",
    "                  optimizer='rmsprop',\n",
    "                  metrics=['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "model_emb = init_model(vocab_size)\n",
    "\n",
    "es = EarlyStopping(patience=5,restore_best_weights=True,monitor='val_accuracy')\n",
    "history_emb = model_emb.fit(X_train_pad, y_train,\n",
    "          validation_split= 2/7,\n",
    "          epochs=100, \n",
    "          batch_size=32, \n",
    "          verbose=1,\n",
    "          use_multiprocessing=True,\n",
    "          callbacks = [es])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('accuracy', model_emb.evaluate(X_test_pad,y_test)[1])\n",
    "plot_loss(history_emb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## build convolutional model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_data_cnn(X_train,X_test):\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    X_train_pad = pad_sequences(X_train, dtype = 'float32',padding='post',maxlen=50)\n",
    "    X__test_pad = pad_sequences(X_test, dtype = 'float32',padding='post',maxlen=50)\n",
    "    \n",
    "    return X_train_pad,X__test_pad\n",
    "\n",
    "X_train_pad_cnn, X_test_pad_cnn = generate_data_cnn(sentences_token_train,sentences_token_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_model_cnn(vocab_size):\n",
    "    \n",
    "    model = Sequential()\n",
    "\n",
    "    model.add(layers.Embedding(input_dim=vocab_size+1, output_dim=100,input_length=50, mask_zero=True))\n",
    "    model.add(layers.Conv1D(16,kernel_size = 5, activation='relu')) \n",
    "    model.add(layers.Flatten()) \n",
    "    model.add(layers.Dense(10, activation='relu'))\n",
    "    model.add(layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "    model.compile(loss= 'binary_crossentropy',  \n",
    "                  optimizer='rmsprop',\n",
    "                  metrics=['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "model_cnn = init_model_cnn(vocab_size)\n",
    "\n",
    "es = EarlyStopping(patience=5,restore_best_weights=True,monitor='val_accuracy')\n",
    "history_cnn = model_cnn.fit(X_train_pad_cnn, y_train,\n",
    "          validation_split= 2/7,\n",
    "          epochs=100, \n",
    "          batch_size=32, \n",
    "          verbose=1,\n",
    "          use_multiprocessing=True,\n",
    "          callbacks = [es])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('accuracy', model_cnn.evaluate(X_test_pad_cnn,y_test)[1])\n",
    "plot_loss(history_cnn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "\n",
    "word2vec = Word2Vec(sentences=sentence_train)\n",
    "vocab_size=len(word2vec.wv.vocab.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def embed_sentence(word2vec, sentence):\n",
    "    embedded=[]\n",
    "    embedded_sentence = []\n",
    "    for word in sentence:\n",
    "         if word in word2vec.wv.vocab.keys():\n",
    "             vector = word2vec.wv[word]\n",
    "             embedded_sentence.append(vector)                 \n",
    "    return np.array(embedded_sentence)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def embedding(word2vec, sentences):\n",
    "    embedding=[]\n",
    "    for sentence in sentences:\n",
    "        embedded_sentence = embed_sentence(word2vec, sentence)  \n",
    "        embedding.append(embedded_sentence)\n",
    "    return embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def embedding_pipeline(word2vec, X):\n",
    "    # Step #3: Sentences to list of words\n",
    "    X = embedding(word2vec, X) \n",
    "    # Step #4: Pad the inputs\n",
    "    X = pad_sequences(X, dtype='float32', padding='post') \n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_pad_w2v = embedding_pipeline(word2vec, sentence_train.values)\n",
    "X__test_pad_w2v = embedding_pipeline(word2vec, sentence_test.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_model_w2v(vocab_size):\n",
    "\n",
    "    model = Sequential()\n",
    "\n",
    "    model.add(layers.Masking(mask_value=0.))\n",
    "    model.add(layers.GRU(units=13, activation='tanh')) \n",
    "    model.add(layers.Dense(64, activation='relu'))\n",
    "#     model.add(layers.Dropout(.2))\n",
    "    model.add(layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "    model.compile(loss= 'binary_crossentropy',  \n",
    "                  optimizer='rmsprop',\n",
    "                  metrics=['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train word2vec on internal data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "model_w2v = init_model_w2v(vocab_size)\n",
    "\n",
    "es = EarlyStopping(patience=5,restore_best_weights=True,monitor='val_accuracy')\n",
    "history_w2v = model_w2v.fit(X_train_pad_w2v, y_train,\n",
    "          validation_split= 2/7,\n",
    "          epochs=50, \n",
    "          batch_size=16, \n",
    "          verbose=1,\n",
    "          use_multiprocessing=True,\n",
    "          callbacks = [es])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('accuracy', model_cnn.evaluate(X_test_pad_w2v,y_test)[1])\n",
    "plot_loss(history_w2v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_w2v.evaluate(X__test_pad_word2v,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train word2vec on Google news"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "import gensim.downloader as api\n",
    "word2vec_gnews = api.load(\"word2vec-google-news-300\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_pad_w2v_gnews = embedding_pipeline(word2vec_gnews, sentence_train[:10000].values)\n",
    "X__test_pad_w2v_gnews = embedding_pipeline(word2vec_gnews, sentence_test[:10000].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "model_w2v_gnews = init_model_w2v(vocab_size)\n",
    "\n",
    "es = EarlyStopping(patience=5,restore_best_weights=True,monitor='val_accuracy')\n",
    "history_w2v_gnews = model_w2v.fit(X_train_pad_w2v_gnews, y_train,\n",
    "          validation_split= 2/7,\n",
    "          epochs=50, \n",
    "          batch_size=16, \n",
    "          verbose=1,\n",
    "          use_multiprocessing=True,\n",
    "          callbacks = [es])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('accuracy', model_cnn.evaluate(X_test_pad_w2v_gnews,y_test)[1])\n",
    "plot_loss(history_w2v_gnews)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('w2v_internal accuracy:',model_w2v.evaluate(X_test_pad_w2v,y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('w2v_gnews accuracy:',model_w2v.evaluate(X_test_pad_w2v_gnews,y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('embeddings+LSTM accuracy:'model_emb.evaluate(X_test_pad,y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('cnn accuracy:',model_cnn.evaluate(X_test_pad_cnn,y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict on London"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nest_asyncio\n",
    "import sys\n",
    "from green_mood_tracker.twint_class import TWINT\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '../green_mood_tracker/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kwargs = dict(\n",
    "    keywords=['nuclear', 'green'],\n",
    "    cities=['London', 'paris'],\n",
    "    since = '2020-11-10 12:00:00',\n",
    "    limit=200,\n",
    "    file_path='../green_mood_tracker/data/london_nuclear.csv'\n",
    ")\n",
    "\n",
    "# t = TWINT(**kwargs)\n",
    "\n",
    "# df_city = t.city_csv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! ls ../green_mood_tracker/data/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv = pd.read_csv('../green_mood_tracker/data/london_nuclear.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
